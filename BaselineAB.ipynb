{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68150d45-40fb-4ea8-b93f-56ffdeac99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, time, random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from decimal import Decimal, InvalidOperation\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import outlines\n",
    "from pydantic import BaseModel, Field, ValidationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e97282-0d88-4f51-8b33-d950f3bb8c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 20250809\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device selection: prefer Apple Metal (MPS) on Mac\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53fc7433-3da2-4898-900a-78bd94b38780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "RUN_NAME = \"gsm8k500_deepseek_r1d_qwen1p5b\"\n",
    "BASE_DIR = Path(\"runs\") / datetime.now().strftime(\"%Y-%m-%d\")\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SUBSET_PATH = BASE_DIR / f\"{RUN_NAME}_indices.json\"  # fixed subset indices\n",
    "A_JSONL = BASE_DIR / f\"{RUN_NAME}_baselineA_thinking.jsonl\"\n",
    "B_JSONL = BASE_DIR / f\"{RUN_NAME}_baselineB_nothinking.jsonl\"\n",
    "\n",
    "def ensure_fixed_subset(dataset, n=500, key_file=SUBSET_PATH):\n",
    "    if key_file.exists():\n",
    "        with open(key_file, \"r\") as f:\n",
    "            idxs = json.load(f)\n",
    "    else:\n",
    "        all_idxs = list(range(len(dataset)))\n",
    "        random.Random(SEED).shuffle(all_idxs)\n",
    "        idxs = sorted(all_idxs[:n])\n",
    "        with open(key_file, \"w\") as f:\n",
    "            json.dump(idxs, f)\n",
    "    return dataset.select(idxs), idxs\n",
    "\n",
    "def parse_gold_gsm8k(answer_text: str) -> Optional[str]:\n",
    "    # GSM8K gold answers end with '#### <num>'\n",
    "    m = re.search(r\"####\\s*([-+]?\\d+(?:\\.\\d+)?)\", answer_text.strip())\n",
    "    return m.group(1).replace(\",\", \"\") if m else None\n",
    "\n",
    "def normalize_number_str(s: Optional[str]) -> Optional[str]:\n",
    "    if s is None: return None\n",
    "    s = s.strip().replace(\",\", \"\")\n",
    "    try:\n",
    "        d = Decimal(s)\n",
    "        s2 = format(d.normalize(), 'f')\n",
    "        if '.' in s2:\n",
    "            s2 = s2.rstrip('0').rstrip('.')\n",
    "        return s2\n",
    "    except (InvalidOperation, ValueError):\n",
    "        return None\n",
    "\n",
    "def equal_numbers(a: Optional[str], b: Optional[str]) -> bool:\n",
    "    na, nb = normalize_number_str(a), normalize_number_str(b)\n",
    "    return (na is not None) and (nb is not None) and (na == nb)\n",
    "\n",
    "def extract_pred_fields(text: str) -> Tuple[Optional[str], Optional[float]]:\n",
    "    m1 = re.search(r\"Final Answer:\\s*([-+]?\\d+(?:\\.\\d+)?)\", text, flags=re.IGNORECASE)\n",
    "    m2 = re.search(r\"Confidence:\\s*(0(?:\\.\\d+)?|1(?:\\.0+)?)\", text, flags=re.IGNORECASE)\n",
    "    ans = m1.group(1) if m1 else None\n",
    "    conf = float(m2.group(1)) if m2 else None\n",
    "    if conf is not None:\n",
    "        conf = max(0.0, min(1.0, conf))\n",
    "    return ans, conf\n",
    "\n",
    "def think_answer_split(text: str) -> Tuple[str, str]:\n",
    "    m = re.search(r\"<think>(.*?)</think>\", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        think = m.group(1)\n",
    "        rest = text[:m.start()] + text[m.end():]\n",
    "        return think, rest\n",
    "    return \"\", text\n",
    "\n",
    "def count_tokens(tokenizer, text: str) -> int:\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d39a2e-b402-4945-9b93-c12f4e374585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B on mps dtype=torch.float16\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "if DEVICE == \"mps\":\n",
    "    DTYPE = torch.float16\n",
    "elif DEVICE == \"cuda\":\n",
    "    DTYPE = torch.bfloat16\n",
    "else:\n",
    "    DTYPE = torch.float32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, trust_remote_code=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\" if DEVICE in (\"cuda\",\"mps\") else None,\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "# Wrap the SAME HF model & tokenizer with outlines (no re-download)\n",
    "omodel = outlines.from_transformers(hf_model, tokenizer)\n",
    "\n",
    "print(f\"Loaded {MODEL_ID} on {DEVICE} dtype={DTYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d45acab9-9a50-4238-9217-6b3071170912",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_BASE = (\n",
    "    \"You are a careful math solver. Solve grade-school math problems reliably.\\n\"\n",
    "    \"You may use <think>...</think> for private reasoning, but evaluation only reads:\\n\"\n",
    "    \"  Final Answer: <number>\\n\"\n",
    "    \"  Confidence: <0-1>\\n\"\n",
    "    \"The confidence should reflect your belief the final answer is correct.\"\n",
    ")\n",
    "\n",
    "USER_INSTR_A = (\n",
    "    \"Solve the problem step by step. You may include <think>...</think> to reason privately.\\n\"\n",
    "    \"When you are done, output exactly two lines:\\n\"\n",
    "    \"Final Answer: <number>\\n\"\n",
    "    \"Confidence: <0-1>\"\n",
    ")\n",
    "\n",
    "USER_INSTR_B = (\n",
    "    \"Solve the problem concisely. DO NOT output any <think> tags.\\n\"\n",
    "    \"Only provide the two final lines in this exact format:\\n\"\n",
    "    \"Final Answer: <number>\\n\"\n",
    "    \"Confidence: <0-1>\"\n",
    ")\n",
    "\n",
    "def build_prompt(question: str, mode: str) -> str:\n",
    "    user_instr = USER_INSTR_A if mode == \"A\" else USER_INSTR_B\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_BASE},\n",
    "        {\"role\": \"user\", \"content\": f\"{user_instr}\\n\\nQuestion:\\n{question}\\n\"}\n",
    "    ]\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    except Exception:\n",
    "        return (\n",
    "            f\"<|system|>\\n{SYSTEM_BASE}\\n<|end|>\\n\"\n",
    "            f\"<|user|>\\n{user_instr}\\n\\nQuestion:\\n{question}\\n<|end|>\\n<|assistant|>\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e89a454-9138-41bc-88bc-6a30809e5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_freeform(prompt: str,\n",
    "                      max_new_tokens: int = 512,\n",
    "                      temperature: float = 0.7,\n",
    "                      top_p: float = 0.95,\n",
    "                      seed: int = SEED) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(hf_model.device)\n",
    "    torch.manual_seed(seed)\n",
    "    with torch.no_grad():\n",
    "        out = hf_model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    # Heuristic to strip the prompt if included in the decode\n",
    "    if decoded.startswith(prompt):\n",
    "        decoded = decoded[len(prompt):]\n",
    "    return decoded\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def batch_iter(lst, bs):\n",
    "    for i in range(0, len(lst), bs):\n",
    "        yield i, lst[i:i+bs]\n",
    "\n",
    "def generate_freeform_batched(prompts: List[str],\n",
    "                              max_new_tokens: int = 512,\n",
    "                              temperature: float = 0.7,\n",
    "                              top_p: float = 0.95,\n",
    "                              seed: int = SEED,\n",
    "                              batch_size: int = 8) -> List[str]:\n",
    "    \"\"\"\n",
    "    Efficient batched generation. Returns only the newly generated suffix for each prompt.\n",
    "    \"\"\"\n",
    "    all_outputs = []\n",
    "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    gen = torch.Generator(device=hf_model.device)\n",
    "    gen.manual_seed(seed)\n",
    "\n",
    "    for offset, batch_prompts in tqdm(list(batch_iter(prompts, batch_size)), desc=\"Generating (batched)\"):\n",
    "        enc = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, add_special_tokens=False)\n",
    "        # True input lengths before padding (per example)\n",
    "        input_lens = enc[\"attention_mask\"].sum(dim=1).tolist()\n",
    "\n",
    "        enc = {k: v.to(hf_model.device) for k, v in enc.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = hf_model.generate(\n",
    "                **enc,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=pad_id,\n",
    "                use_cache=True,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=False,\n",
    "            )\n",
    "\n",
    "        seqs = out.sequences  # [B, input_len + new_len]\n",
    "        # Slice off the prompt portion per item using true input lengths\n",
    "        for i in range(seqs.size(0)):\n",
    "            new_tokens = seqs[i, input_lens[i]:]\n",
    "            text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "            all_outputs.append(text)\n",
    "    return all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b216c1-5612-467d-9bf2-a9e45e4506ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typed schema for hard-constrained decoding\n",
    "class TailSchema(BaseModel):\n",
    "    final_answer: float\n",
    "    confidence: float = Field(..., ge=0.0, le=1.0)\n",
    "\n",
    "def constrained_tail_with_outlines(question: str,\n",
    "                                   mode: str,\n",
    "                                   max_new_tokens: int = 64,\n",
    "                                   temperature: float = 0.2,\n",
    "                                   top_p: float = 0.95,\n",
    "                                   seed: int = SEED) -> Tuple[Optional[str], Optional[float], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Returns (final_answer_str, confidence_float, raw_json_string) using outlines typed decoding.\n",
    "    Produces ONLY the structured object; we later format 'Final Answer:' and 'Confidence:' lines ourselves.\n",
    "    \"\"\"\n",
    "    # A compact instruction that avoids generating thoughts; we just want the numbers.\n",
    "    inst = (\n",
    "        \"Return only a JSON object with two fields:\\n\"\n",
    "        '{ \"final_answer\": <number>, \"confidence\": <number between 0 and 1> }.\\n'\n",
    "        \"No text or explanation.\"\n",
    "    )\n",
    "    if mode == \"A\":\n",
    "        style = \"You may think privately first, but do not output the thoughts.\"\n",
    "    else:\n",
    "        style = \"Be concise and do not include any hidden or private reasoning.\"\n",
    "\n",
    "    prompt = f\"{inst}\\n\\nQuestion:\\n{question}\\n\\n{style}\"\n",
    "    torch.manual_seed(seed)\n",
    "    raw = omodel(prompt, TailSchema, max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p)\n",
    "    try:\n",
    "        parsed = TailSchema.model_validate_json(raw)\n",
    "        ans_str = normalize_number_str(str(parsed.final_answer))\n",
    "        conf = float(parsed.confidence)\n",
    "        conf = max(0.0, min(1.0, conf))\n",
    "        return ans_str, conf, raw\n",
    "    except ValidationError as e:\n",
    "        return None, None, raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa364a5a-1881-4b8b-b583-62adac2552e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSM8K test size: 1319\n",
      "Using fixed subset of size: 500\n",
      "Saved indices to: runs/2025-08-09/gsm8k500_deepseek_r1d_qwen1p5b_indices.json\n"
     ]
    }
   ],
   "source": [
    "gsm = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "gsm500, indices = ensure_fixed_subset(gsm, n=10, key_file=SUBSET_PATH)\n",
    "\n",
    "print(\"GSM8K test size:\", len(gsm))\n",
    "print(\"Using fixed subset of size:\", len(gsm500))\n",
    "print(\"Saved indices to:\", SUBSET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17730cab-68d9-4fb7-a0cd-d034cdd56953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(split,\n",
    "                  mode: str,\n",
    "                  out_path: Path,\n",
    "                  max_new_tokens_free: int,\n",
    "                  temperature_free: float,\n",
    "                  top_p_free: float,\n",
    "                  desc: str):\n",
    "    \"\"\"\n",
    "    mode: 'A' (thinking) or 'B' (no thinking)\n",
    "    - Freeform pass to capture raw + <think>\n",
    "    - Parse 'Final Answer'/'Confidence'\n",
    "    - If missing/invalid, do a constrained repair using outlines typed decoding\n",
    "    - Log JSONL rows\n",
    "    \"\"\"\n",
    "    out_f = open(out_path, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    correct_count = 0\n",
    "    conf_sum, conf_n = 0.0, 0\n",
    "    repaired_ct = 0\n",
    "\n",
    "    pbar = tqdm(total=len(split), desc=desc)\n",
    "    for i, ex in enumerate(split):\n",
    "        q = ex[\"question\"]\n",
    "        gold_raw = ex[\"answer\"]\n",
    "        gold = parse_gold_gsm8k(gold_raw)\n",
    "\n",
    "        prompt = build_prompt(q, mode=mode)\n",
    "\n",
    "        t0 = time.time()\n",
    "        gen = generate_freeform(\n",
    "            prompt=prompt,\n",
    "            max_new_tokens=max_new_tokens_free,\n",
    "            temperature=temperature_free,\n",
    "            top_p=top_p_free,\n",
    "            seed=SEED + i\n",
    "        )\n",
    "        latency_ms = int((time.time() - t0) * 1000)\n",
    "\n",
    "        # Parse from freeform\n",
    "        pred_ans, pred_conf = extract_pred_fields(gen)\n",
    "\n",
    "        # If parsing failed, get a hard-constrained tail with outlines (latest API)\n",
    "        tail_raw = None\n",
    "        tail_forced = False\n",
    "        if (pred_ans is None) or (pred_conf is None):\n",
    "            tail_forced = True\n",
    "            repaired_ct += 1\n",
    "            ans_fix, conf_fix, raw_json = constrained_tail_with_outlines(\n",
    "                question=q, mode=mode, seed=SEED + i\n",
    "            )\n",
    "            tail_raw = raw_json\n",
    "            if ans_fix is not None:\n",
    "                pred_ans = ans_fix\n",
    "            if conf_fix is not None:\n",
    "                pred_conf = conf_fix\n",
    "\n",
    "        # Final guards\n",
    "        if pred_conf is not None:\n",
    "            pred_conf = max(0.0, min(1.0, float(pred_conf)))\n",
    "\n",
    "        is_correct = equal_numbers(pred_ans, gold)\n",
    "\n",
    "        # Token accounting (<think> vs rest)\n",
    "        think_text, _ = think_answer_split(gen)\n",
    "        tok_total = count_tokens(tokenizer, gen)\n",
    "        tok_think = count_tokens(tokenizer, think_text) if think_text else 0\n",
    "        tok_answer = tok_total - tok_think\n",
    "\n",
    "        row = {\n",
    "            \"id\": f\"gsm8k_test_{indices[i]}\",\n",
    "            \"dataset\": \"gsm8k\",\n",
    "            \"split\": \"test\",\n",
    "            \"index\": indices[i],\n",
    "            \"prompt_version\": f\"mode_{mode}\",\n",
    "            \"model\": MODEL_ID,\n",
    "            \"quant\": None,\n",
    "            \"seed\": SEED + i,\n",
    "            \"temps\": {\"temperature\": temperature_free, \"top_p\": top_p_free},\n",
    "            \"setting\": {\n",
    "                \"device\": DEVICE,\n",
    "                \"dtype\": str(DTYPE),\n",
    "                \"max_new_tokens_free\": max_new_tokens_free,\n",
    "                \"hard_tail_via_outlines\": tail_forced\n",
    "            },\n",
    "            \"final_answer\": pred_ans,\n",
    "            \"confidence\": pred_conf,\n",
    "            \"gold\": gold,\n",
    "            \"correct\": bool(is_correct),\n",
    "            \"tok_counts\": {\n",
    "                \"generated_total\": tok_total,\n",
    "                \"think\": tok_think,\n",
    "                \"answer\": tok_answer\n",
    "            },\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"raw_freeform\": gen,\n",
    "            \"raw_tail_outlines_json\": tail_raw\n",
    "        }\n",
    "        out_f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # live stats\n",
    "        correct_count += int(is_correct)\n",
    "        if pred_conf is not None:\n",
    "            conf_sum += pred_conf\n",
    "            conf_n += 1\n",
    "\n",
    "        acc_so_far = correct_count / (i + 1)\n",
    "        avg_conf_so_far = (conf_sum / conf_n) if conf_n else 0.0\n",
    "        pbar.set_postfix(acc=f\"{acc_so_far:.3f}\", mean_conf=f\"{avg_conf_so_far:.3f}\",\n",
    "                         repaired=repaired_ct, t_ms=latency_ms)\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    out_f.close()\n",
    "\n",
    "    final_acc = correct_count / len(split)\n",
    "    final_mean_conf = (conf_sum / conf_n) if conf_n else float(\"nan\")\n",
    "    print(f\"\\nSaved -> {out_path}\")\n",
    "    print(f\"Final Accuracy: {final_acc:.4f}  |  Mean Confidence: {final_mean_conf:.4f}  |  Repairs: {repaired_ct}/{len(split)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee4a4d9-bf6a-4590-b910-91b1c6dfd401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark_batched(split,\n",
    "                          mode: str,\n",
    "                          out_path: Path,\n",
    "                          max_new_tokens_free: int,\n",
    "                          temperature_free: float,\n",
    "                          top_p_free: float,\n",
    "                          batch_size: int = 8,\n",
    "                          desc: str = \"Benchmark (batched)\"):\n",
    "    \"\"\"\n",
    "    2-step per item:\n",
    "      1) Batched freeform gen to capture raw (and potential <think>).\n",
    "      2) If parsing fails, repair that item with typed, hard-constrained decoding via outlines.\n",
    "    \"\"\"\n",
    "    # Build prompts up-front so lengths are known and batching is simple\n",
    "    prompts = [build_prompt(ex[\"question\"], mode=mode) for ex in split]\n",
    "    questions = [ex[\"question\"] for ex in split]\n",
    "    golds_raw = [ex[\"answer\"] for ex in split]\n",
    "    golds = [parse_gold_gsm8k(a) for a in golds_raw]\n",
    "\n",
    "    t0 = time.time()\n",
    "    gen_texts = generate_freeform_batched(\n",
    "        prompts,\n",
    "        max_new_tokens=max_new_tokens_free,\n",
    "        temperature=temperature_free,\n",
    "        top_p=top_p_free,\n",
    "        seed=SEED,              # reproducible if order & batch size stay the same\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    t1 = time.time()\n",
    "\n",
    "    assert len(gen_texts) == len(split)\n",
    "\n",
    "    out_f = open(out_path, \"w\", encoding=\"utf-8\")\n",
    "    correct_count, conf_sum, conf_n, repaired_ct = 0, 0.0, 0, 0\n",
    "\n",
    "    pbar = tqdm(total=len(split), desc=desc)\n",
    "    for i in range(len(split)):\n",
    "        q = questions[i]\n",
    "        gold = golds[i]\n",
    "        gen = gen_texts[i]\n",
    "\n",
    "        # Parse freeform\n",
    "        pred_ans, pred_conf = extract_pred_fields(gen)\n",
    "\n",
    "        # Hard-constrained repair (typed) if needed\n",
    "        tail_forced = False\n",
    "        tail_raw = None\n",
    "        if (pred_ans is None) or (pred_conf is None):\n",
    "            tail_forced = True\n",
    "            repaired_ct += 1\n",
    "            ans_fix, conf_fix, raw_json = constrained_tail_with_outlines(\n",
    "                question=q, mode=mode, seed=SEED + i\n",
    "            )\n",
    "            tail_raw = raw_json\n",
    "            if ans_fix is not None:\n",
    "                pred_ans = ans_fix\n",
    "            if conf_fix is not None:\n",
    "                pred_conf = conf_fix\n",
    "\n",
    "        if pred_conf is not None:\n",
    "            pred_conf = max(0.0, min(1.0, float(pred_conf)))\n",
    "\n",
    "        is_correct = equal_numbers(pred_ans, gold)\n",
    "\n",
    "        # Token accounting for process metrics\n",
    "        think_text, _ = think_answer_split(gen)\n",
    "        tok_total = count_tokens(tokenizer, gen)         # new tokens only (suffix)\n",
    "        tok_think = count_tokens(tokenizer, think_text) if think_text else 0\n",
    "        tok_answer = max(0, tok_total - tok_think)\n",
    "\n",
    "        row = {\n",
    "            \"id\": f\"gsm8k_test_{indices[i]}\",\n",
    "            \"dataset\": \"gsm8k\",\n",
    "            \"split\": \"test\",\n",
    "            \"index\": indices[i],\n",
    "            \"prompt_version\": f\"mode_{mode}\",\n",
    "            \"model\": MODEL_ID,\n",
    "            \"quant\": None,\n",
    "            \"seed\": SEED + i,\n",
    "            \"temps\": {\"temperature\": temperature_free, \"top_p\": top_p_free},\n",
    "            \"setting\": {\n",
    "                \"device\": DEVICE,\n",
    "                \"dtype\": str(DTYPE),\n",
    "                \"max_new_tokens_free\": max_new_tokens_free,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"hard_tail_via_outlines\": tail_forced\n",
    "            },\n",
    "            \"final_answer\": pred_ans,\n",
    "            \"confidence\": pred_conf,\n",
    "            \"gold\": gold,\n",
    "            \"correct\": bool(is_correct),\n",
    "            \"tok_counts\": {\n",
    "                \"generated_total\": tok_total,\n",
    "                \"think\": tok_think,\n",
    "                \"answer\": tok_answer\n",
    "            },\n",
    "            # per-item latency not meaningful in batched mode; include batch wall time instead\n",
    "            \"latency_ms\": None,\n",
    "            \"raw_freeform\": gen,\n",
    "            \"raw_tail_outlines_json\": tail_raw\n",
    "        }\n",
    "        out_f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # live stats\n",
    "        correct_count += int(is_correct)\n",
    "        if pred_conf is not None:\n",
    "            conf_sum += pred_conf\n",
    "            conf_n += 1\n",
    "\n",
    "        acc_so_far = correct_count / (i + 1)\n",
    "        avg_conf_so_far = (conf_sum / conf_n) if conf_n else 0.0\n",
    "        pbar.set_postfix(acc=f\"{acc_so_far:.3f}\", mean_conf=f\"{avg_conf_so_far:.3f}\",\n",
    "                         repaired=repaired_ct, batches_time_s=f\"{(t1-t0):.1f}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    out_f.close()\n",
    "\n",
    "    final_acc = correct_count / len(split)\n",
    "    final_mean_conf = (conf_sum / conf_n) if conf_n else float(\"nan\")\n",
    "    print(f\"\\nSaved -> {out_path}\")\n",
    "    print(f\"Final Accuracy: {final_acc:.4f} | Mean Confidence: {final_mean_conf:.4f} | Repairs: {repaired_ct}/{len(split)}\")\n",
    "    print(f\"Batched generation wall time: {(t1 - t0):.2f}s for {len(split)} items (bs={batch_size})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7babb9c3-4166-4913-9cc3-2305e11bf44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline A (thinking, batched)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45cd642bbf24253b3c906bc226fc8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating (batched):   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m BATCH_SIZE = \u001b[32m8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBaseline A (thinking, batched)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mrun_benchmark_batched\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgsm500\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mA_JSONL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens_free\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature_free\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEMP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p_free\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOP_P\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBaseline A (batched)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBaseline B (no thinking, batched)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m run_benchmark_batched(\n\u001b[32m     19\u001b[39m     split=gsm500,\n\u001b[32m     20\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     desc=\u001b[33m\"\u001b[39m\u001b[33mBaseline B (batched)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mrun_benchmark_batched\u001b[39m\u001b[34m(split, mode, out_path, max_new_tokens_free, temperature_free, top_p_free, batch_size, desc)\u001b[39m\n\u001b[32m     18\u001b[39m golds = [parse_gold_gsm8k(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m golds_raw]\n\u001b[32m     20\u001b[39m t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m gen_texts = \u001b[43mgenerate_freeform_batched\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens_free\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature_free\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p_free\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# reproducible if order & batch size stay the same\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m t1 = time.time()\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(gen_texts) == \u001b[38;5;28mlen\u001b[39m(split)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mgenerate_freeform_batched\u001b[39m\u001b[34m(prompts, max_new_tokens, temperature, top_p, seed, batch_size)\u001b[39m\n\u001b[32m     48\u001b[39m enc = {k: v.to(hf_model.device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m enc.items()}\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     out = \u001b[43mhf_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m seqs = out.sequences  \u001b[38;5;66;03m# [B, input_len + new_len]\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Slice off the prompt portion per item using true input lengths\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/self_doubt_cot/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/self_doubt_cot/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2634\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2626\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2627\u001b[39m         input_ids=input_ids,\n\u001b[32m   2628\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2629\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2630\u001b[39m         **model_kwargs,\n\u001b[32m   2631\u001b[39m     )\n\u001b[32m   2633\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2634\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2640\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2642\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2645\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2646\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2647\u001b[39m         input_ids=input_ids,\n\u001b[32m   2648\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2649\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2650\u001b[39m         **model_kwargs,\n\u001b[32m   2651\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/self_doubt_cot/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:3660\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3658\u001b[39m     probs = nn.functional.softmax(next_token_scores, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m   3659\u001b[39m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3660\u001b[39m     next_tokens = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m   3661\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3662\u001b[39m     next_tokens = torch.argmax(next_token_scores, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "TEMP = 0.7\n",
    "TOP_P = 0.95\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "print(\"Baseline A (thinking, batched)...\")\n",
    "run_benchmark_batched(\n",
    "    split=gsm500,\n",
    "    mode=\"A\",\n",
    "    out_path=A_JSONL,\n",
    "    max_new_tokens_free=512,\n",
    "    temperature_free=TEMP,\n",
    "    top_p_free=TOP_P,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    desc=\"Baseline A (batched)\"\n",
    ")\n",
    "\n",
    "print(\"\\nBaseline B (no thinking, batched)...\")\n",
    "run_benchmark_batched(\n",
    "    split=gsm500,\n",
    "    mode=\"B\",\n",
    "    out_path=B_JSONL,\n",
    "    max_new_tokens_free=256,\n",
    "    temperature_free=TEMP,\n",
    "    top_p_free=TOP_P,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    desc=\"Baseline B (batched)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a53081-de8e-4b1e-b322-ac40038fa653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_jsonl(path: Path):\n",
    "    n = 0\n",
    "    correct = 0\n",
    "    conf_sum = 0.0\n",
    "    conf_n = 0\n",
    "    repaired = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            n += 1\n",
    "            obj = json.loads(line)\n",
    "            correct += int(bool(obj.get(\"correct\", False)))\n",
    "            c = obj.get(\"confidence\", None)\n",
    "            if isinstance(c, (int, float)):\n",
    "                conf_sum += float(c)\n",
    "                conf_n += 1\n",
    "            if obj.get(\"setting\", {}).get(\"hard_tail_via_outlines\", False):\n",
    "                repaired += 1\n",
    "    acc = correct / n if n else float(\"nan\")\n",
    "    mean_conf = conf_sum / conf_n if conf_n else float(\"nan\")\n",
    "    print(f\"{path.name}: n={n} | accuracy={acc:.4f} | mean_conf={mean_conf:.4f} | constrained_repairs={repaired}\")\n",
    "\n",
    "summarize_jsonl(A_JSONL)\n",
    "summarize_jsonl(B_JSONL)\n",
    "\n",
    "print(\"\\nArtifacts:\")\n",
    "print(\"Fixed subset indices:\", SUBSET_PATH)\n",
    "print(\"Baseline A JSONL:\", A_JSONL)\n",
    "print(\"Baseline B JSONL:\", B_JSONL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
